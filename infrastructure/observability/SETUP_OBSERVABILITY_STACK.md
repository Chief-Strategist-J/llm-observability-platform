# Deep Dive: Observability Stack Architecture & Setup

This comprehensive guide details the design, configuration, and automation of the Scaibu Observability Stack. It is designed for engineers who need to understand *why* and *how* every component works, and how to modify it safely.

---

## üèóÔ∏è 1. Architecture Overview

 The stack runs on a dedicated Docker network (`172.28.0.0/16`) with static IPs to ensure stable service discovery without relying on Docker's internal DNS for critical scrape targets.

| Service | IP Address | Port | Function |
|---------|------------|------|----------|
| **OTEL Collector** | `172.28.0.10` | 4317/8888 | Universal telemetry receiver & processor |
| **Prometheus** | `172.28.0.20` | 9090 | Metrics TSDB & Scraping Engine |
| **Loki** | `172.28.0.30` | 3100 | Log Aggregation System |
| **Jaeger** | `172.28.0.40` | 16686 | Distributed Tracing UI & Backend |
| **Alertmanager** | `172.28.0.50` | 9093 | Alert deduplication & routing |
| **Grafana** | `172.28.0.60` | 3000 | Visualization Dashboard |

---

## üìÑ 2. Configuration File Deep Dive

### 2.1 [observability-dynamic-docker.yaml](file:///home/j/live/dinesh/llm-chatbot-python/infrastructure/observability/setup/config/observability-dynamic-docker.yaml)
**Purpose**: Defines the Docker services, resource limits, and network topology.
- **Networks**: Uses an `external` network `observability-network`. This prevents Docker Compose from creating a new isolated network, allowing us to manage IPs manually via Python.
- **Resources**: Strict limits (e.g., `512m` memory) prevent one service from crashing the host.
- **Traefik Labels**: Every service has `traefik.enable: "true"` and `Host('scaibu.svc')` rules. This enables HTTPS access via `https://scaibu.grafana` instead of `http://localhost:3000`.

### 2.2 [otel-config.yaml](file:///home/j/live/dinesh/llm-chatbot-python/infrastructure/observability/setup/config/otel-config.yaml)
**Purpose**: Configures the OpenTelemetry Collector.
- **Receivers**:
  - `otlp`: Listens on 4317 (gRPC) and 4318 (HTTP) for app telemetry.
  - `filelog`: Reads Docker container logs from `/var/lib/docker/containers/*/*.log`. It parses the JSON/Time format automatically.
- **Exporters**:
  - `loki`: Pushes logs to `http://172.28.0.30:3100`.
  - `prometheusremotewrite`: Pushes metrics to Prometheus.
  - `otlp/jaeger`: Forwards traces to Jaeger.

### 2.3 [prometheus.yml](file:///home/j/live/dinesh/llm-chatbot-python/infrastructure/observability/setup/config/prometheus.yml)
**Purpose**: Scrape configuration for Prometheus.
- **Scrape Jobs**:
  - `otel-collector-apps` (Port 8889): Scrapes metrics generated by your Python apps sent via OTLP.
  - `otel-collector` (Port 8888): Scrapes the collector's *internal* health metrics.
  - `docker-containers`: Uses `docker_sd_configs` to auto-discover other containers.
- **Alerting**: Points to `alerting-rules.yml` and sends alerts to `172.28.0.50:9093`.

### 2.4 [loki-config.yaml](file:///home/j/live/dinesh/llm-chatbot-python/infrastructure/observability/setup/config/loki-config.yaml)
**Purpose**: Configures Loki storage and limits.
- **Storage**: Uses local filesystem (`/loki/chunks`). In production, you would switch this to S3.
- **Retention**: Set to `720h` (30 days).
- **Ingestion**: Limits set to 10MB/sec to prevent OOM kills.

### 2.5 [alerting-rules.yml](file:///home/j/live/dinesh/llm-chatbot-python/infrastructure/observability/setup/config/alerting-rules.yml)
**Purpose**: Defines when to fire alerts.
- `ServiceDown`: Fires if `up == 0`.
- `HighMemoryUsage`: Fires if memory > 90%.
- `TestAlert`: A manual trigger (`test_gauge > 5`) used for verification.

### 2.6 [grafana-datasources.yml](file:///home/j/live/dinesh/llm-chatbot-python/infrastructure/observability/setup/config/grafana-datasources.yml)
**Purpose**: Pre-provisions datasources so you don't have to add them manually.
- **UIDs**: Explicitly set to `prometheus`, `loki`, `jaeger`. This allows dashboards to be portable (they reference UID, not database ID).

---

## ü§ñ 3. Automation Scripts Deep Dive

The setup is orchestrated by **Temporal** workflows to ensure reliability and retries.

### 3.1 Activities ([observability_stack_setup_activities.py](file:///home/j/live/dinesh/llm-chatbot-python/infrastructure/observability/setup/observability_stack_setup_activities.py))
This file contains the "atomic" units of work:
1.  **`create_observability_network_activity`**: checks if `observability-network` exists; if not, creates it with subnet `172.28.0.0/16`.
2.  **`start_observability_stack_activity`**: Runs `docker-compose up -d`. It verifies the file exists first.
3.  **`verify_observability_stack_activity`**: Iterates through all containers (`otel-collector`, `grafana`, etc.) and checks `docker inspect` status. returns `success: False` if any are missing.

### 3.2 Workflow ([observability_stack_setup_workflow.py](file:///home/j/live/dinesh/llm-chatbot-python/infrastructure/observability/setup/observability_stack_setup_workflow.py))
Stateful orchestration logic:
1.  **Dependencies**: Runs `create_observability_network_activity` BEFORE `start_observability_stack_activity`.
2.  **Certificates**: Calls `generate_certificates_activity` (from shared orchestrator) to create valid SSL certs for `scaibu.grafana`, etc.
3.  **Traefik**: Generates `traefik_dynamic_tls.yaml` so Traefik knows where the certs are.
4.  **Network**: Allocates Virtual IPs (`allocate_virtual_ips_activity`) and updates `/etc/hosts` (`add_hosts_entries_activity`) so you can access `https://scaibu.grafana` locally.
5.  **Verification**: Waits 10s then verifies stack health.

### 3.3 Worker ([observability_stack_setup_worker.py](file:///home/j/live/dinesh/llm-chatbot-python/infrastructure/observability/setup/observability_stack_setup_worker.py))
Connects to Temporal Server and listens on `observability-setup-queue`. It registers the workflow and activities so they can be executed.

### 3.4 Trigger ([trigger_observability_stack_setup.py](file:///home/j/live/dinesh/llm-chatbot-python/infrastructure/observability/setup/trigger_observability_stack_setup.py))
A CLI entrypoint. It connects to Temporal and sends the `StartWorkflow` command.

---

## üõ†Ô∏è 4. Common Operations & Commands

### Start the Stack
```bash
# 1. Start the Worker (Needs to be running)
python infrastructure/observability/setup/observability_stack_setup_worker.py

# 2. Trigger the Setup (In a new terminal)
python infrastructure/observability/setup/trigger_observability_stack_setup.py setup
```

### Stop/Teardown
This will remove containers, network, and clean up `/etc/hosts`.
```bash
python infrastructure/observability/setup/trigger_observability_stack_setup.py teardown
```

### Manual Debugging
If automation fails, you can intervene manually:
```bash
cd infrastructure/observability/setup
docker-compose -f config/observability-dynamic-docker.yaml ps
docker-compose -f config/observability-dynamic-docker.yaml logs -f otel-collector
```

### Modifying Configuration
1.  Edit the file in `infrastructure/observability/setup/config/`.
2.  Restart the specific service:
    ```bash
    docker-compose -f config/observability-dynamic-docker.yaml restart prometheus
    ```
    *(Note: For `otel-collector` or `loki` config changes, a restart is required)*

---

## ‚ùì 5. Troubleshooting FAQ

**Q: "Datasource not found" in Grafana?**
A: This happens if volumes persist old data.
**Fix**: Run `docker volume rm config_grafana-data` and restart Grafana.

**Q: 404 Page Not Found on `scaibu.grafana`?**
A: Traefik routing issue.
**Fix**: Ensure `observability_stack_setup_workflow.py` successfully ran `add_hosts_entries_activity` and `allocate_virtual_ips_activity`. Check `/etc/hosts` for `127.0.1.1 scaibu.grafana`.

**Q: No Logs in Loki?**
A: Check OTEL Collector permissions. It needs to read `/var/lib/docker`. The container runs as `0:0` (root) to ensure access. Verify with `docker logs otel-collector`.
